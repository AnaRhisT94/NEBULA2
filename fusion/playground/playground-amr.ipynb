{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4422f0-1ada-4324-8173-2fb89a605b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "import amrlib\n",
    "import penman\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "# import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79742a6-1df7-40ae-aec8-c3a3960e81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.getcwd()+'/../..')  # /home/gil/dev/NEBULA2/\n",
    "os.chdir(os.getcwd()+'/../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41181f5b-01c9-4ff4-82bf-8819720dc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebula_api.nebula_enrichment_api import *\n",
    "from experts.common.RemoteAPIUtility import RemoteAPIUtility\n",
    "from nebula_api.vlmapi import VLM_API\n",
    "from nebula_api.atomic2020.comet_enrichment_api import *\n",
    "from nebula_api.canonisation_api import CANON_API\n",
    "# from nlp_tools.light_house_generator import LightHouseGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9fe9d6-598c-499a-b08f-b548dcc3fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nebula_api.playground_api as pg_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddd9364-8b09-4c79-9fdc-e9329d40e5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n",
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14']\n",
      "Milvus Collection Loaded:  vcomet_mdmmt_embedded_event\n",
      "Milvus Collection Loaded:  vcomet_mdmmt_embedded_place\n",
      "Milvus Collection Loaded:  vcomet_mdmmt_embedded_actions\n",
      "INFO:tensorflow:Restoring parameters from /app/NEBULA2/nebula_api/mdmmt_api/ckpts/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /app/NEBULA2/nebula_api/mdmmt_api/ckpts/vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available class names: ['clip_vit', 'clip_rn', 'mdmmt_max', 'mdmmt_mean', 'mdmmt_legacy']\n"
     ]
    }
   ],
   "source": [
    "nre = NRE_API()\n",
    "api = RemoteAPIUtility()\n",
    "vlm = VLM_API()\n",
    "# mdmmt = mdmmt_api.MDMMT_API()\n",
    "# comet = Comet(\"/app/NEBULA2/nebula_api/atomic2020/comet-atomic_2020_BART\")\n",
    "ascore = CANON_API()\n",
    "stog = amrlib.load_stog_model(model_dir=\"/app/NEBULA2/models/model_stog\")\n",
    "gtos = amrlib.load_gtos_model(model_dir=\"/app/NEBULA2/models/model_gtos\")\n",
    "# lh_gen = LightHouseGenerator(comet,stog,gtos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5dcd694-7362-4aa8-89e8-a6107f71ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Movies/114206952',\n",
    "'Movies/114207205',\n",
    "'Movies/114207398',\n",
    "'Movies/114207499',\n",
    "'Movies/114207361',\n",
    "'Movies/114207740',\n",
    "'Movies/114207908',\n",
    "'Movies/114208744',\n",
    "'Movies/114206724',\n",
    "'Movies/114206548',\n",
    "'Movies/114206264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e631f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "from IPython.display import HTML, display\n",
    "import base64\n",
    "\n",
    "\n",
    "def download_video_file(movie, fname='/tmp/video_file.mp4'):    \n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "    query = 'FOR doc IN Movies FILTER doc._id == \"{}\" RETURN doc'.format(movie)\n",
    "    cursor = api.db.aql.execute(query)\n",
    "    url_prefix = \"http://ec2-18-159-140-240.eu-central-1.compute.amazonaws.com:7000/\"\n",
    "    url_link = ''\n",
    "    for doc in cursor:\n",
    "        url_link = url_prefix+doc['url_path']\n",
    "        url_link = url_link.replace(\".avi\", \".mp4\")   \n",
    "        print(url_link)\n",
    "        urllib.request.urlretrieve(url_link, fname) \n",
    "    return fname\n",
    "    # video = cv2.VideoCapture(self.temp_file)\n",
    "    # fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    # return(fps, url_link)\n",
    "\n",
    "\n",
    "\n",
    "def read_video_segm(abspath, t_beg, t_end):\n",
    "    cmd = f'ffmpeg -y -ss {t_beg} -i {abspath} -max_muxing_queue_size 9999  -loglevel error -f mp4 -vf scale=\"(floor(112/ih * iw/2))*2:112\"  -c:a copy  -movflags frag_keyframe+empty_moov -t {t_end - t_beg} pipe:1 -nostats -hide_banner -nostdin'\n",
    "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    assert p.returncode == 0, cmd\n",
    "    buf = p.stdout\n",
    "    return buf\n",
    "\n",
    "video_id_cnt = 0    \n",
    "class VideoElem:\n",
    "    def __init__(self, fname, t_start=0, t_end=999):\n",
    "        with open(fname, 'rb') as f:\n",
    "            #data = base64.standard_b64encode(f.read())\n",
    "            buf = read_video_segm(fname, t_start, t_end)\n",
    "            data = base64.standard_b64encode(buf)\n",
    "        global video_id_cnt\n",
    "        video_id_cnt += 1\n",
    "        self.video_id_cnt = video_id_cnt\n",
    "        elem = HTML(f\"\"\"\n",
    "            <video id=\"video_{self.video_id_cnt}\" autoplay loop muted>\n",
    "                <source src=\"data:video/mp4;base64,{data.decode('ascii')}\" type=\"video/mp4\">\n",
    "            </video>        \n",
    "        \"\"\")\n",
    "        display(elem)\n",
    "    \n",
    "    def hide(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").hide()'\n",
    "        display(Javascript(js))\n",
    "        \n",
    "    def show(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").show()'\n",
    "        display(Javascript(js))\n",
    "\n",
    "    def remove(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").remove()'\n",
    "        display(Javascript(js))\n",
    "        \n",
    "def mdmmt_video_encode(start_f, stop_f, path='/tmp/video_file.mp4', freq=24):\n",
    "        t_start = start_f//freq\n",
    "        t_end = stop_f//freq\n",
    "        if t_start == t_end:\n",
    "            t_start = t_start - 1\n",
    "        print(\"Start/stop\", t_start, \" \", t_end)\n",
    "        if (t_end - t_start) >= 1:\n",
    "            vemb = mdmmt.encode_video(\n",
    "                mdmmt.vggish_model,  # adio modality\n",
    "                mdmmt.vmz_model,  # video modality\n",
    "                mdmmt.clip_model,  # image modality\n",
    "                mdmmt.model_vid,  # aggregator\n",
    "                path, t_start, t_end)\n",
    "            return(vemb)\n",
    "        else:\n",
    "            print(\"Stage too short\")\n",
    "            return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb78a56f-a2e5-40f5-8e85-2d94d3f357a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_concepts(frame)-> List:\n",
    "    def transform_concept(c):\n",
    "        exp = re.compile(r\"^([a-zA-z]+)(\\d*)$\")\n",
    "        r = exp.match(c)\n",
    "        return r.group(1) if r else c\n",
    "        \n",
    "    pre_concepts = set(frame['tracker_description']).union(set(frame['step_description'])).union(set(frame['simulated_expert']))\n",
    "    concepts = list(set(map(transform_concept,pre_concepts)))\n",
    "    return concepts\n",
    "\n",
    "def kgbart_fusion(frames) -> (List[str], List[str]):\n",
    "    h, outname = tempfile.mkstemp(text=True)\n",
    "    os.close(h)\n",
    "    h, fname = tempfile.mkstemp(text=True)\n",
    "    os.close(h)\n",
    "    KGBART_MAIN = BASE_DIR+'/kgbart/KGBART/KGBART_training/decode_seq2seq.py'\n",
    "    KGBART_CC_DIR = BASE_DIR+'/kgbart/downloaded/commongen_dataset'\n",
    "    KGBART_MODEL_DIR = BASE_DIR+'/kgbart/output/best_model/model.best.bin'\n",
    "    options = {\n",
    "        'data_dir': KGBART_CC_DIR,\n",
    "        'output_dir': os.path.dirname(outname),\n",
    "        'input_file': fname,\n",
    "        'model_recover_path': KGBART_MODEL_DIR,\n",
    "        'output_file': os.path.basename(outname),\n",
    "        'split': 'dev',\n",
    "        'beam_size': 5,\n",
    "        'forbid_duplicate_ngrams': True\n",
    "    }\n",
    "    all_concepts = []\n",
    "    with open(fname, 'w') as f:\n",
    "        for frame in frames:\n",
    "            concepts = frame_to_concepts(frame)\n",
    "            all_concepts.append(', '.join(concepts))\n",
    "            f.write(' '.join(concepts)+'\\n')\n",
    "        \n",
    "    # write expert tokens to input file\n",
    "    \n",
    "    cmdline = 'python '+KGBART_MAIN+' '+ ' '.join(['--{} {}'.format(k,v) for (k,v) in options.items()]) + '>/dev/null 2>&1'\n",
    "    os.system(cmdline)\n",
    "    with open(outname,'r') as f:\n",
    "        rc = f.readlines()\n",
    "    os.unlink(outname)\n",
    "    os.unlink(fname)\n",
    "    return all_concepts, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2243e402-1e54-466f-bf5a-e930ccc6d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = vlm.encode_text(texts, **kwargs)\n",
    "    if type(emb_batch) == list:\n",
    "        emb_batch = torch.stack(emb_batch,axis=0)\n",
    "    if normalize:\n",
    "        video_emb = video_emb / video_emb.norm(2)\n",
    "        # print(\"normalized video norm: {}\".format(video_emb.norm(2)))\n",
    "        n = (emb_batch * emb_batch).sum(axis=1).sqrt()\n",
    "        emb_batch = emb_batch / n.unsqueeze(1).expand_as(emb_batch)\n",
    "        # print(\"normalized text norms:\")\n",
    "        # for emb in emb_batch:\n",
    "        #     print(emb.norm(2))                        \n",
    "    return (video_emb.unsqueeze(0).expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_concept(c):\n",
    "    exp = re.compile(r\"^([a-zA-z]+)-?(\\d*)$\")\n",
    "    r = exp.match(c)\n",
    "    return r.group(1) if r else c\n",
    "\n",
    "class ConceptManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def ground_concept(concept):\n",
    "        return transform_concept(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5004e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityManager:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def similarity(self, c1, c2):\n",
    "        if type(c2) is not list:\n",
    "            c2 = [c2]   \n",
    "        a = self.nlp(c1)\n",
    "        targets = self.nlp(' '.join(c2))\n",
    "        return [a.similarity(x) for x in targets]\n",
    "\n",
    "\n",
    "smanager = SimilarityManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b13d497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "class SubsetOptimization:\n",
    "    def __init__(self, video_emb, experts: List, candidates_strings: List[str]):\n",
    "        self.stog = amrlib.load_stog_model(model_dir=\"/app/NEBULA2/models/model_stog\")\n",
    "        self.video_emb = video_emb\n",
    "        self.initial_temp = 10\n",
    "        self.final_temp = .05\n",
    "        self.alpha = 0.01\n",
    "        self.theta = 0.5\n",
    "        self.experts = experts\n",
    "        self.candidates_strings = candidates_strings\n",
    "        self.candidates_amr_strings = self.stog.parse_sents(self.candidates_strings) \n",
    "        self.candidates = self.candidates_amr_strings\n",
    "        self.candidates_amrs = [penman.decode(x) for x in self.candidates_amr_strings]\n",
    "        self.candidates_similarity = compute_batch_scores(self.video_emb, self.candidates_strings)             \n",
    "        self.opt_results = []\n",
    "        self.smanager = SimilarityManager()\n",
    "\n",
    "        self.coverage_matrix = np.zeros([len(self.experts),len(self.candidates)])\n",
    "        self.coverage_matrix[:] = np.nan\n",
    "        for i in range(len(experts)):\n",
    "            for j in range(len(candidates_strings)):\n",
    "                self.coverage_matrix[i][j]=self.concept_amr_similarity(self.experts[i],self.candidates_amrs[j])\n",
    "        self.max_size = int(len(self.experts)*1.5)\n",
    "\n",
    "    def concept_amr_similarity(self, concept, amr):\n",
    "        insts = [ConceptManager.ground_concept(x.target) for x in amr.instances()]\n",
    "        sims = self.smanager.similarity(concept, insts)\n",
    "        return max(sims)\n",
    "\n",
    "    def get_coverage(self,i,j):        \n",
    "        if np.isnan(self.coverage_matrix[i][j]):\n",
    "            self.coverage_matrix[i][j] = self.concept_amr_similarity(self.experts[i],self.candidates_amrs[j])\n",
    "        return self.coverage_matrix[i][j]\n",
    "\n",
    "    def get_expert_coverage(self,state):\n",
    "        return self.coverage_matrix[:,state].max(axis=1)\n",
    "\n",
    "    def get_state_coverage(self,state) -> float:\n",
    "        print(\"State coverage for {}:\".format(state))\n",
    "        print(self.get_expert_coverage(state))\n",
    "        return np.mean(self.get_expert_coverage(state))\n",
    "\n",
    "    # def get_state_coverage(self, state: List[int]) -> float:\n",
    "    #     experts_coverage = [max([self.get_coverage(i,j) for j in state]) for i in range(len(self.experts))]    # A list of partial coverege        \n",
    "    #     return sum(experts_coverage) / len(self.experts)\n",
    "\n",
    "    def get_cost(self, state: List[int]) -> float:\n",
    "        if not state:\n",
    "            return 0\n",
    "        coverage_score = self.get_state_coverage(state)           \n",
    "        similarity_score = self.candidates_similarity[state].mean().item()\n",
    "        return -(coverage_score + self.theta*similarity_score)\n",
    "\n",
    "    # state here is assumed (and guaranteed on return) to be -sorted-\n",
    "    def get_candidate(self, state: List[int]) -> List[int]:\n",
    "        def compute_state_arrays(s):\n",
    "            print(\"Computing arrays for state: \")\n",
    "            print(s)\n",
    "            s_score = self.candidates_similarity[s]\n",
    "            s_coverage = self.coverage_matrix.mean(axis=0)[s]\n",
    "            s_max_coverage = self.coverage_matrix.max(axis=0)[s]\n",
    "            s_fitscore = s_coverage+self.theta*s_score\n",
    "\n",
    "            return (s_score,s_coverage,s_max_coverage,s_fitscore)\n",
    "\n",
    "        if not state:\n",
    "            print(\"Empty state\")\n",
    "            return [random.randint(0,len(self.candidates_strings)-1)]\n",
    "            \n",
    "        rc = state.copy()\n",
    "        s = np.array(state)\n",
    "        s_score, s_coverage, s_max_coverage, s_fitscore = compute_state_arrays(s)\n",
    "               \n",
    "        if len(state) == self.max_size:\n",
    "            print(\"Maximum state size, removing\")\n",
    "            idx = np.argmin(s_fitscore)\n",
    "            del rc[idx]\n",
    "            return rc\n",
    "            \n",
    "        remove_sentence = random.random()<self.get_state_coverage(state)        \n",
    "        print(\"coverage of {} is {}, remove?{}\".format(state,self.get_state_coverage(state),remove_sentence))\n",
    "        if remove_sentence:             # We decide to remove a sentence from the set\n",
    "            print(\"Removing\")\n",
    "            probs = softmax(-s_fitscore)\n",
    "            idx = np.random.multinomial(1,probs).argmax()\n",
    "            del rc[idx]                   \n",
    "        else:                           # Add a sentence from the outside\n",
    "            print(\"Adding\")\n",
    "            anti_state = []\n",
    "            for i in range(len(self.candidates_strings)):\n",
    "                if not i in state:\n",
    "                    anti_state.append(i)\n",
    "            s1 = np.array(anti_state)\n",
    "            s1_score, s1_coverage, s1_max_coverage, s1_fitscore = compute_state_arrays(s1)\n",
    "            # Pick an expert to try and cover\n",
    "            probs = softmax(self.get_expert_coverage(s)*10)         # Coverage is in (0,1), so we use low temprature\n",
    "            expert_to_cover = np.random.multinomial(1,probs).argmax()\n",
    "            probs = softmax(self.coverage_matrix[expert_to_cover][s1]*10)\n",
    "            idx_to_add = np.random.multinomial(1,probs).argmax()\n",
    "            bisect.insort(rc,anti_state[idx_to_add])\n",
    "            \n",
    "        return rc\n",
    "\n",
    "\n",
    "\n",
    "    def get_scored_permutations(self, k):\n",
    "        n = len(self.candidates)\n",
    "        return [(x,self.get_cost(list(x))) for x in itertools.permutations(range(n),k)]\n",
    "        \n",
    "    def simulated_annealing(self, initial_state):\n",
    "        self.opt_results = []\n",
    "        current_temp = self.initial_temp\n",
    "\n",
    "       # Start by initializing the current state with the initial state\n",
    "        current_state = initial_state\n",
    "\n",
    "        while current_temp > self.final_temp:\n",
    "            next_cand = self.get_candidate(current_state)\n",
    "\n",
    "            print(\"current cost: {} ({}). Candidate cost: {} ({})\".format(self.get_cost(current_state),current_state,self.get_cost(next_cand),next_cand))\n",
    "\n",
    "            # Check if next_cand is best so far\n",
    "            cost_diff = self.get_cost(current_state) - self.get_cost(next_cand)\n",
    "\n",
    "            # if the new solution is better, accept it\n",
    "            if cost_diff > 0:\n",
    "                current_state = next_cand\n",
    "            # if the new solution is not better, accept it with a probability of e^(-cost/temp)\n",
    "            else:\n",
    "                print(\"chance to move: {}\".format(math.exp(cost_diff / current_temp)))\n",
    "                if random.uniform(0, 1) < math.exp(cost_diff / current_temp):\n",
    "                    current_state = next_cand\n",
    "            # decrement the temperature\n",
    "            current_temp -= self.alpha\n",
    "            self.opt_results.append(-self.get_cost(current_state))\n",
    "\n",
    "        return current_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb8a2c1-993e-479d-ab98-47572ac0452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concepts(mid, scene_elem, use_db=False):\n",
    "    if use_db:\n",
    "        return nre.get_groundings_from_db(mid, scene_elem)\n",
    "    \n",
    "    concepts, attributes, persons, triplets, verbs = lh_gen.decompose_lighthouse(events=events, actions=[],\n",
    "                                                                             places=places)\n",
    "    concepts = flatten(concepts.values())\n",
    "    attributes = flatten(attributes.values())\n",
    "    triplets = flatten(triplets.values())\n",
    "    persons = flatten(persons.values())\n",
    "    \n",
    "    return concepts, attributes, persons, triplets, verbs['verbs']\n",
    "\n",
    "# get sets of concepts and triplets and return set of amrs/sentences\n",
    "\n",
    "def generate_candidates(concepts, attributes, persons, triplets, verbs):\n",
    "    return lh_gen.generate_from_concepts(concepts, attributes, persons, triplets, verbs,\n",
    "                                                    places, None)\n",
    "    \n",
    "# def fusion_pipeline(mid: str, scene_elem: int, **kwargs):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7bd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get a list of 1-item dictionaries, return a list of the values\n",
    "'''\n",
    "\n",
    "def rearrange_concepts(concepts):\n",
    "    return [list(x.values())[0] for x in concepts]\n",
    "\n",
    "def permute_sentence(sentence, concepts):    \n",
    "    def replace_instance(g: penman.Graph, changes: List[tuple[int,str]]) -> penman.Graph :\n",
    "        amr_copy = penman.Graph(triples=g.triples, epidata=g.epidata)\n",
    "        for (i,val) in changes:\n",
    "            b = list(amr_copy.triples[i])\n",
    "            b[2] = val\n",
    "            amr_copy.triples[i] = tuple(b)\n",
    "        return amr_copy\n",
    "\n",
    "    concepts = {k: rearrange_concepts(v) for (k,v) in concepts.items()}\n",
    "    s = re.sub('Person[XYZ]', 'man', sentence.strip())\n",
    "    # s = re.sub('[0-9]+', 'man', sentence.strip())\n",
    "    # s = re.sub('___', 'man', x.strip())\n",
    "    print(\"Original Sentence: {}\".format(s))\n",
    "    [amr] = stog.parse_sents([s])\n",
    "    pen = penman.decode(amr)\n",
    "    insts_list = []\n",
    "    rc = []\n",
    "    dims = []\n",
    "    for i,triplet in enumerate(pen.triples):\n",
    "        if triplet[1] == ':instance':\n",
    "            entity_class = ascore.get_class_of_entity(transform_concept(triplet[2]))\n",
    "            if entity_class == 'none':                          # This instance has no class, so we create its own special class to take care of the edge case\n",
    "                entity_class = 'none{}'.format(i)\n",
    "                concepts[entity_class] = []\n",
    "            if triplet[2] not in concepts[entity_class]:\n",
    "                concepts[entity_class].append(triplet[2])\n",
    "            insts_list.append((i,triplet, entity_class))\n",
    "            dims.append(range(len(concepts[entity_class])))\n",
    "    prods = itertools.product(*dims)\n",
    "    for cand in prods:        \n",
    "        changes = [(insts_list[i][0],concepts[insts_list[i][2]][d]) for (i,d) in enumerate(cand)]\n",
    "        rc.append(replace_instance(pen,changes))\n",
    "    \n",
    "    return pen, rc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6337b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ec2-18-159-140-240.eu-central-1.compute.amazonaws.com:7000/static/development/1031_Quantum_of_Solace_00_52_35_159-00_52_37_144.mp4\n",
      "Movie info: {'arango_id': 'Movies/114206849', 'description': '1031_Quantum_of_Solace_00_52_35_159-00_52_37_144', 'fps': 23, 'width': 1920, 'height': 1080, 'last frame': 300, 'movie_id': '5f22092fd8e94613b142fed64ae0225e', 'mdfs': [[2, 18, 34], [39, 45, 44]], 'scene_elements': [[0, 36], [36, 48]]}\n",
      "fn path: /tmp/video_file.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 7.44 GiB total capacity; 6.30 GiB already allocated; 141.50 MiB free; 6.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_332/280389684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# video = VideoElem(download_video_file(mid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# movie_info = api.get_movie_info(mid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memb_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mdmmt_mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# movie_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/NEBULA2/nebula_api/vlmapi.py\u001b[0m in \u001b[0;36mencode_video\u001b[0;34m(self, mid, scene_element, class_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mvid_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdmmt_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvggish_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmz_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mdmmt_mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mvid_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdmmt_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvggish_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmz_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mdmmt_legacy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mvid_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdmmt_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_video_legacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvggish_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmz_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_vid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/NEBULA2/nebula_api/mdmmt_api/mdmmt_api.py\u001b[0m in \u001b[0;36mencode_video\u001b[0;34m(self, vggish_model, vmz_model, clip_model, model_vid, path, t_start, t_end, fps, encode_type)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNoAudio\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mtimings_vggish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membs_vggish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         timings_vmz, embs_vmz = self.visual_compute_embs(vmz_model, path, t_start, t_end,\n\u001b[0m\u001b[1;32m    253\u001b[0m                                                     fps=fps, frames_per_clip=32, frame_crop_size=224, frame_size=224)\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/NEBULA2/nebula_api/mdmmt_api/mdmmt_api.py\u001b[0m in \u001b[0;36mvisual_compute_embs\u001b[0;34m(self, model, path, t_start, t_end, fps, frame_size, frame_crop_size, per_batch_size, frames_per_clip)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0membs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/NEBULA2/nebula_api/mdmmt_api/models/vmz_model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch_imgs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torchvision/models/video/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/playground/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB (GPU 0; 7.44 GiB total capacity; 6.30 GiB already allocated; 141.50 MiB free; 6.66 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "mid = 'Movies/114206849' # movies[0]\n",
    "# events, places = comet.get_playground_data(mid, 0)\n",
    "# video = VideoElem(download_video_file(mid))\n",
    "# movie_info = api.get_movie_info(mid)\n",
    "emb_image = vlm\n",
    "\n",
    ".encode_video(mid,0,class_name='mdmmt_mean')\n",
    "# movie_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a89052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [x[0] for x in z4[-10:]]\n",
    "sentences = ['woman in the alleyway',\n",
    "             'woman alley outside apartments',\n",
    "             'woman enters the narrow street',\n",
    "             'woman need find a dark street corner',\n",
    "             'woman the dark street corner',\n",
    "             'woman need to find the alley way',\n",
    "             'woman sees the friend across the street',\n",
    "             'woman need to find a narrow street',\n",
    "             'woman narrow street',\n",
    "             'woman  the alleyway']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nre.get_groundings_from_db(mid,0)\n",
    "data['triplets'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf0833-1f5a-4cdc-afbd-6a207e4ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '4 waits by the baggage claim'\n",
    "data['triplets'][s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbe78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pen, rc = permute_sentence(' PersonX walks to the baggage claim',data['concepts'][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528c42d-c416-4490-9f78-19ebdd54b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d72a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bla = gtos.generate([penman.encode(x) for x in rc[3000:6000]])\n",
    "# penman.encode(pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea75663-26e9-4f59-b64a-eea497acb40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bla[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e433a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_batch_scores(emb_image, bla[0])\n",
    "orig_score = compute_batch_scores(emb_image, ['man walks to the baggage claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573d282-57ee-421c-b5f7-271a9f82a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argpartition(scores, -10)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b36b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(bla[0][i], scores[i]) for i in ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef9658-bdd7-4ea7-abe3-d230f2a76e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_batch_scores(emb_image,['Bellboys go to leave the slate.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a245485",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a36ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = [(x,ascore.get_class_of_entity(x)) for x in [\"stand\", \"sit\", \"take\", \"give\", \"look\", \"see\", \"smile\", \"eat\", \"hold\", \"drink\", \"put\", \"think\", \"comprehend\", \"speak\", \"talk\", \"raise\", \"pick\", \"announce\", \"laugh\", \"run\", \"walk\"]]\n",
    "print('-------------------------------------------------------')\n",
    "print(rc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006c7ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SubsetOptimization(emb_image, [\"woman\", \"friend\", \"dark\", \"hurry\"], sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rc = opt.get_scored_permutations(3)\n",
    "rc = opt.simulated_annealing([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(x=(range(len(opt.opt_results))),y=opt.opt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d6efa-38a3-4034-b57d-5b659abe14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = nre.get_groundings_from_db(mid, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd4bc9-610f-4104-984e-803f35c5636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc['verbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts, attributes, persons, triplets, verbs = lh_gen.decompose_lighthouse(events=events, actions=[],\n",
    "                                                                             places=places)\n",
    "\n",
    "concepts = flatten(concepts.values())\n",
    "attributes = flatten(attributes.values())\n",
    "triplets = flatten(triplets.values())\n",
    "persons = flatten(persons.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5e983-da6c-463a-bd7c-9449a9de54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = [x.strip() for x in [' suitcase', ' car', ' car seat']]\n",
    "attributes = [x.strip() for x in [' hurriedly', ' rushed', ' hurried']]\n",
    "persons = [x.strip() for x in [' gets hit by a car', ' to get in the car', ' to get to the car']]\n",
    "triplets = [x.strip() for x in [' PersonX gets hit by a car',\n",
    " ' PersonX steps off of the curb',\n",
    " ' PersonX is running late for work']]\n",
    "verbs = {}\n",
    "verbs['verbs'] = [x.strip() for x in ['get', 'accept', 'acquire']]\n",
    "places = [x.strip() for x in ['on a narrow street or alley',\n",
    " 'on a narrow street',\n",
    " 'outside in a large alley']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sent = lh_gen.generate_from_concepts(concepts[:3], attributes[:3], persons[:3], triplets[:3], verbs['verbs'][:3],\n",
    "                                                    places[:3], None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = comet.get_groundings(events,places,type=\"triplet\")\n",
    "for c in cands.keys():\n",
    "    texts = cands[c]\n",
    "    cands[c] = [x for x in texts if x.strip()]\n",
    "# compute_batch_scores(embamrs = stog.parse_sents(sentences)_image,[\"woman hurriedly steps off of the curb into the street carrying her luggage to the car\", \"hurriedly steps out of the street\", \"hurriedly steps out of the street carrying her luggage to the car\"])\n",
    "# compute_batch_scores(emb_image,[\"man is wearing a wide-brimmed hat\", \"man is wearing a wide - brimmed floral hat\", \"man wants to take off their hat\", \"man with a hat\"])\n",
    "# compute_batch_scores(emb_image,[\"woman hurriedly steps off of the curb into the street carrying her luggage to the car\", \"woman hurriedly steps out of the street\", \"woman hurriedly steps out of the street carrying her luggage to the car\"])\n",
    "diffs = []\n",
    "orig_c = []\n",
    "best_c = []\n",
    "all_output = []\n",
    "all_scores = []\n",
    "inp_scores = []\n",
    "for c in cands.keys():\n",
    "    # print('working on: {}'.format(c))\n",
    "    final_cands = [re.sub('PersonX', 'woman', x.strip()) for x in cands[c]]\n",
    "    final_c = re.sub('PersonX', 'woman', c.strip())\n",
    "    orig_score = compute_batch_scores(emb_image, [final_c])\n",
    "    rc = compute_batch_scores(emb_image, final_cands)\n",
    "    best_score = rc.max()\n",
    "    ind = rc.cpu().numpy().argmax()\n",
    "    all_output.append(final_cands)\n",
    "    all_scores.append(rc)\n",
    "    orig_c.append(final_c)\n",
    "    inp_scores.append(orig_score)\n",
    "    best_c.append(final_cands[ind])\n",
    "    diffs.append(rc.max()-orig_score[0])\n",
    "    # print(\"orig score for {}: {}. Max score after: {}. Diff: {}\".format(final_c,orig_score,rc.max(), rc.max()-orig_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3eefb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "['{} -> {} : {}'.format(x,y,z.item()) for (x,y,z) in zip(orig_c,best_c,diffs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = [x for l in all_output for x in l]\n",
    "z2 = torch.concat(all_scores)\n",
    "z3 = list(zip(z1,[round(x,4) for x in z2.cpu().tolist()]))\n",
    "z3 = list(set(z3))\n",
    "z4 = sorted(z3,key=lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.candidates_similarity.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f901a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemgetter(*list(reversed(sorted(rc,key = lambda x: x[1])))[0][0])(opt.candidates_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223be31-5f6b-40e1-8340-ab99a19d6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "[b, c, d] = stog.parse_sents(['He comes from New York', 'man sits in a chair waiting for someone', \"he drives carefully\"])\n",
    "a = penman.decode(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = list(a.triples[-3])\n",
    "b[2] = \"love\"\n",
    "a.triples[-3] = tuple(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83177554",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = penman.Graph(triples=a.triples,\n",
    "      epidata=a.epidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca896b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1.triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05841e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.triples.index(a._filter_triples('c', ':instance', None)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6635ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "penman.encode(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ec550",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtos.generate([penman.encode(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555690bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_amr_similarity(\"male\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "z = nlp('dog cat milk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "z[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bc53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtos = amrlib.load_gtos_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd1ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = stog.parse_sents(['The woman looks at a rabid dog as it bites an old, well-dressed man',\n",
    "                     'I was late to the airport and almost missed my flight'])\n",
    "gtos.generate(z)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e59dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascore.get_concept_from_entity('friendship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d65537",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConceptManager.ground_concept('bite-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9705fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_giltest('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0dd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "WordNetLemmatizer().lemmatize('chairs', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ac6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_movies = comet.get_playground_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in all_movies:\n",
    "    df = pg_api.get_or_create_normalized_video(m)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "BASE_DIR = os.path.abspath(os.getcwd()+'/../..')  # /home/gil/dev/NEBULA2/\n",
    "os.chdir(os.getcwd()+'/../..')\n",
    "from nebula_api.mdmmt_api import mdmmt_api\n",
    "mdmmt = mdmmt_api.MDMMT_API()\n",
    "def mdmmt_video_encode(start_f, stop_f, path='/tmp/video_file.mp4', freq=24):\n",
    "    t_start = start_f//freq\n",
    "    t_end = stop_f//freq\n",
    "    if t_start == t_end:\n",
    "        t_start = t_start - 1\n",
    "    print(\"Start/stop\", t_start, \" \", t_end)\n",
    "    if (t_end - t_start) >= 1:\n",
    "        vemb = mdmmt.encode_video(\n",
    "            mdmmt.vggish_model,  # adio modality\n",
    "            mdmmt.vmz_model,  # video modality\n",
    "            mdmmt.clip_model,  # image modality\n",
    "            mdmmt.model_vid,  # aggregator\n",
    "            path, t_start, t_end)\n",
    "        return(vemb)\n",
    "    else:\n",
    "        print(\"Stage too short\")\n",
    "        return(None)\n",
    "emb_image = mdmmt_video_encode(0,48)        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dfe4d185a1b3661f8d189d2dcb52f070789ba26e5d1ea6f8391b638319fa460"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
